{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will work through the different steps of a linear regression workflow. The notebook will walk you through building a first simple model and improving upon that model by stepwise iteration.\n",
    "\n",
    "### 1. First Simple Model\n",
    "- Load in the dataset: inspect the overall shape, duplicate entries, and na's.\n",
    "- Identify the continuous target variable\n",
    "- Perform Initial EDA: correlation plots\n",
    "- Build a FSM (First Simple Model) with statsmodels/Build a FSM with sklearn\n",
    "- Check the assumptions of linear regression  \n",
    "\n",
    "### 2. Iterate: Build a better model - Add another numerical feature\n",
    "- Add another feature, this time with high negative correlation, and fit the model\n",
    "- Compare metrics and interpret coefficients\n",
    "- Check the assumptions\n",
    "\n",
    "### 3. Iterate: Build a better model - Add a categorical feature\n",
    "- Add a categorical variable \n",
    "- Compare metrics and interpret coefficients\n",
    "- Check the assumptions once-again\n",
    "\n",
    "### 4. Appendix\n",
    "- Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "We will use a dataset from [Kaggle](https://www.kaggle.com/kumarajarshi/life-expectancy-who). It contains data collected by the WHO about life expectancy and potentially-related factors.  The information is aggregated on a per-country per-year basis.\n",
    "\n",
    "The following questions have been posed. Read them and keep them in your mind when building your model.  We will reference them as we proceed through the workflow.\n",
    "\n",
    "1. Do various predicting factors which have been chosen initially really affect life expectancy? Which predicting variables actually affect life expectancy?\n",
    "2. Should a country having a lower life expectancy value(<65) increase its healthcare expenditure in order to improve its average lifespan?\n",
    "3. How do infant and adult mortality rates affect life expectancy?\n",
    "4. Does life expectancy have positive or negative correlation with eating habits, lifestyle, exercise, smoking, drinking alcohol etc.\n",
    "5. What is the impact of schooling on the lifespan of humans?\n",
    "6. Does Life expectancy have positive or negative relationship with drinking alcohol?\n",
    "7. Do densely populated countries tend to have lower life expectancy?\n",
    "8. What is the impact of immunization coverage on life Expectancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. FSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the dataset and check the overall shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the dataset\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records are in the data set?\n",
    "records = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many columns are in the dataset?\n",
    "columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate entries\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for na's (just look to get an idea; don't drop or impute yet)\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# 0. Load in the dataset and check the overall shape\n",
    "\n",
    "# load in the dataset\n",
    "df = pd.read_csv('data/life_expectancy.csv')\n",
    "\n",
    "# How many records are in the data set?\n",
    "records = df.shape[0]\n",
    "\n",
    "# How many columns are in the dataset?\n",
    "columns = len(df.columns)\n",
    "\n",
    "# Check for duplicate entries\n",
    "df.duplicated().sum()\n",
    "\n",
    "# Check for na's (just look to get an idea; don't drop or impute yet)\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a row in the dataframe represent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\"\"\"\n",
    "Each row represents a *year* of a *country's* health data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the continous target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Identify the continuous target variable of interest\n",
    "df['Life expectancy ']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had problems isolating that variable, don't worry.  That is expected! \n",
    "There can be odd, burdensome inconsistencies in naming of data.\n",
    "Let's use our Python skills to wipe out the naming inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the column names. \n",
    "There are many ways to do this. One way of doing so, outlined below, uses the columns attribute of the dataframe.  Then, using a list comprehension or for loop, we can manipulate the column name strings using methods that come with the string class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gather column names into a variable\n",
    "columns = None\n",
    "\n",
    "# 2. Strip whitespace from the ends\n",
    "columns = None\n",
    "\n",
    "# 3. Replace white space with underscores\n",
    "columns = None\n",
    "\n",
    "# 4. Make all columns characters lowercase\n",
    "columns = None\n",
    "\n",
    "# 5. Reset the column names of the dataframe\n",
    "# df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure the changes are reflected in the dataset\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, to make things easier to interpet, set the target to column index 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Lastly, to make things easier to interpet, set the target to column index 0\n",
    "\n",
    "cols = list(df.columns)\n",
    "cols = [cols[3]] + cols[:3] + cols[4:]\n",
    "df = df[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "# 1. Gather column names into a variable\n",
    "columns = df.columns\n",
    "\n",
    "# 2. Strip whitespace from the ends\n",
    "columns = [column.strip() for column in columns]\n",
    "\n",
    "# 3. Replace white space with underscores\n",
    "columns = [column.replace(' ', '_') for column in columns]\n",
    "\n",
    "# 4. Make all columns characters lowercase\n",
    "columns = [column.lower() for column in columns]\n",
    "\n",
    "# 5. Reset the column names of the dataframe\n",
    "df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisit the continuous target variable.  \n",
    "# Explore it a bit.  Plot a histogram of its distribution as well as a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Revisit the continuous target variable.  \n",
    "# Explore it a bit.  Plot it's distribution and boxplot\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(10,5))\n",
    "sns.distplot(df.life_expectancy, ax = ax[0])\n",
    "sns.boxplot(df.life_expectancy, ax= ax[1])\n",
    "\n",
    "ax[0].set_title('Distribution of Target Variable: Life Expectancy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('life_expectancy').head(10)\n",
    "df[df.country=='Haiti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the distribution of the target\n",
    "# Look at the min value? What happened in Haiti in 2010?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "print(f'''Generally normal with a left skew.  \n",
    "            Mean of {round(df.life_expectancy.mean(),2)}\n",
    "             Median of {round(df.life_expectancy.median(),2)}\n",
    "             Skew: {round(stats.skew(df.life_expectancy.dropna()), 2)}\n",
    "             ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Initial EDA\n",
    "\n",
    "There are a lot of variables here!  Let's look at a correlation matrix to see which ones might be the most useful.  (Here we are looking for variables that are highly correlated with the target variable, but not highly correlated with other input variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation matrix\n",
    "# first, just use the datafram .corr() method to output a numerical matrix\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then pass the above code into Seaborn's heatmap plot\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try adding the code in this cell to the mask attribute in the heatmap to halve the plot\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# create a correlation matrix\n",
    "# first, just use the datafram .corr() method to output a numerical matrix\n",
    "df.corr()\n",
    "\n",
    "# Then pass the above code into Seaborn's heatmap plot\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(df.corr(), mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judging from the correlation matrix or the heatmap, which three features have the highest positive correlation? \n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# From the heatmap, which features have high correlation? \n",
    "\"Schooling, income_composition_of_resources, BMI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use seaborn's pairplot function on the three features above plus life_expectancy.  \n",
    "Note: we would usually start right off by using a pairplot, but because we have so many features, the pairplot would be unwieldy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "high_correlation_df = df[['life_expectancy', 'schooling', \n",
    "                         'income_composition_of_resources', 'bmi']]\n",
    "\n",
    "sns.pairplot(high_correlation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the top row of the pairplot, one feature's correlation to the target is a bit fuzzier than the rest. \n",
    "Inspecting other cells of the pairplot, the other two features show covariance. \n",
    "Given those two insights, choose one feature to build the First Simple Model with. (Our FSM will be simple the target and one predictor).\n",
    "Consider also whether choosing one of the positively correlated features above the others would help answer any of the question listed at the beginning of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "\"\"\"\n",
    "It looks like the correlation with BMI is a little fuzzier than the others, \n",
    "so let's exclude it for now.  \n",
    "`Schooling` and `Income_Composition_of_Resources` are highly correlated with both life expectancy and each other, \n",
    "so let's only include one of them. \n",
    "`Schooling` seems like a good choice because it would allow us to answer Question 5.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSM with Statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "# Create a dataframe with only the target and the chosen high-positive corellation feature\n",
    "fsm_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this FSM, simply dropnas.\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the R-style formula. The format is \"target~feature_1 + feature_2 + feature_3\"\n",
    "formula = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the dataframe composed of the two features\n",
    "fsm = ols(formula=formula, data=fsm_df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "from statsmodels.formula.api import ols\n",
    "# Create a dataframe with only the target and the chosen high-positive corellation feature\n",
    "\n",
    "fsm_df = df[[\"schooling\", \"life_expectancy\"]].copy()\n",
    "fsm_df.dropna(inplace=True)\n",
    "\n",
    "# build the R-style formula. The format is \"target~feature_1 + feature_2 + feature_3\"\n",
    "\n",
    "formula = \"life_expectancy ~ schooling\"\n",
    "\n",
    "# Fit the model on the dataframe composed of the two features\n",
    "\n",
    "fsm = ols(formula=formula, data=fsm_df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the summary() method on the fsm varaible to print out the results of the fit.\n",
    "fsm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The object also has attributes associated with the ouput, such as: rsquared, and params.\n",
    "# save those values to the variables below.\n",
    "\n",
    "rsquared = None\n",
    "params = None\n",
    "\n",
    "print(f'Rsquared of FSM: {rsquared}')\n",
    "print('----------')\n",
    "print('Beta values of FSM:')\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# The object also has attributes associated with the ouput, such as: rsquared, and params.\n",
    "# save those values to the variables below.\n",
    "\n",
    "rsquared = fsm.rsquared\n",
    "params = fsm.params\n",
    "\n",
    "print(f'Rsquared of FSM: {rsquared}')\n",
    "print('----------')\n",
    "print('Beta values of FSM:')\n",
    "print(params)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the result of the FSM.  What does the R Squared tell you? Remember the formula for:\n",
    "\n",
    "$\\Large R^2 = 1 - \\frac{SSE}{SST}$\n",
    "\n",
    "Also, interepret the coefficients.  If we increase the value of our independent variable by 1, what does it mean for our predicted value?\n",
    "\n",
    "What will our model predict the value of Life Expectancy to be for a country with 0 years of school on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "'''\n",
    "Our R_2 is not too bad. We are only explaining about 57% of the variance in life expectancy, but we only have one feature so far and it's statistically significant at an alpha of 0.05.\n",
    "\n",
    "We could stop right now and say that according to our model:\n",
    "\n",
    " - A country with zero years of schooling on average is expected to have a life expectancy of 44.1 years\n",
    " - For each additional average year of schooling, we expect life expectancy to increase by 2.1 years\n",
    " \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the assumptions of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linearity\n",
    "\n",
    "Linear regression assumes that the input variable linearly predicts the output variable.  We already qualitatively checked that with a scatter plot.  But it's also a good idea to use a statistical test.  This one is the [Rainbow test](https://www.tandfonline.com/doi/abs/10.1080/03610928208828423) which is available from the [diagnostic submodule of StatsModels](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.linear_rainbow.html#statsmodels.stats.diagnostic.linear_rainbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import linear_rainbow, het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "rainbow_statistic, rainbow_p_value = linear_rainbow(fsm)\n",
    "print(\"Rainbow statistic:\", rainbow_statistic)\n",
    "print(\"Rainbow p-value:\", rainbow_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis is that the model is linearly predicted by the features, alternative hypothesis is that it is not.  Thus returning a low p-value means that the current model violates the linearity assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Normality\n",
    "\n",
    "Linear regression assumes that the residuals are normally distributed.  It is possible to check this qualitatively with a Q-Q plot.  The fit model object has an attribute called resid, which is an array of the difference between predicted and true values.  Store the residuals in the variable below, show the qq plot, and interepret. You are looking for the theoretical quantiles and the sample quantiles to line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a qq-plot\n",
    "\n",
    "fsm_resids = None\n",
    "\n",
    "import statsmodels.api as sm\n",
    "sm.qqplot(fsm_resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Create a qq-plot\n",
    "\n",
    "fsm_resids = fsm.resid\n",
    "\n",
    "import statsmodels.api as sm\n",
    "sm.qqplot(fsm_resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those qqplots don't look so good in the upper right corner. To pass a visual test, the qq should be a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Jarque-Bera](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test) test is performed automatically as part of the model summary output, labeled **Jarque-Bera (JB)** and **Prob(JB)**.\n",
    "\n",
    "The null hypothesis is that the residuals are normally distributed, alternative hypothesis is that they are not.  \n",
    "What does the JB score output indicate. Does it support the qq-plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "'''The JB score has a low p-value means that the current model violates the normality assumption. \n",
    "That supports the qq visual with the crooked tail.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Homoscadasticity\n",
    "\n",
    "Linear regression assumes that the variance of the dependent variable is homogeneous across different values of the independent variable(s).  We can visualize this by looking at the predicted life expectancy vs. the residuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the predict() method now available to be called from the fsm variable to store the predictions\n",
    "y_hat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot y_hat against the residuals (stored in fsm_resids) in a scatter plot\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interepret the result. Do you see any patterns that suggest that the residuals exhibit heteroscedasticity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "y_hat = fsm.predict()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(y_hat, fsm_resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "'''\n",
    "Just visually inspecting this, it seems like our model over-predicts life expectancy \n",
    "between 60 and 70 years old in a way that doesn't happen for other age groups.  \n",
    "Plus we have some weird-looking data in the lower end that we might want to inspect.  \n",
    "Maybe there was something wrong with recording those values, \n",
    "or maybe there is something we can feature engineer once we have more independent variables.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also run a statistical test.  The [Breusch-Pagan test](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test) is available from the [diagnostic submodule of StatsModels](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.het_breuschpagan.html#statsmodels.stats.diagnostic.het_breuschpagan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you chose schooling, this should run without error\n",
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(fsm_resids, fsm_df[[\"schooling\"]])\n",
    "print(\"Lagrange Multiplier p-value:\", lm_p_value)\n",
    "print(\"F-statistic p-value:\", f_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis is homoscedasticity, alternative hypothesis is heteroscedasticity.  \n",
    "What does the p-value returned above indicate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "'''Thus returning a low p-value means that the current \n",
    "model violates the homoscedasticity assumption'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Independence\n",
    "\n",
    "The independence assumption means that the independent variables must not be too collinear.  Right now we have only one independent variable, so we don't need to check this yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model with sklearn\n",
    "The sklearn interface is simpler than Statsmodels, but it does not give us the super helpful statsmodel output.  We will, however, use its syntax consistently with other algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit an sklearn model\n",
    "#instantiate a linear regression object \n",
    "lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into target and features\n",
    "y = None\n",
    "X = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call .fit from the linear regression object, and feed X and y in as parameters\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr has a method called score.  Again, feed in X and y, and read the output. Save it in the variable score. \n",
    "# What is that number?  Compare it to statsmodels. \n",
    "score = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr also has attributes coef_ and intercept_. Save and compare to statsmodels\n",
    "beta = None\n",
    "intercept = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit an sklearn model\n",
    "#instantiate a linear regression object \n",
    "lr = LinearRegression()\n",
    "\n",
    "# split the data into target and features\n",
    "y = fsm_df.life_expectancy\n",
    "X = fsm_df.drop('life_expectancy', axis=1)\n",
    "\n",
    "# Call .fit from the linear regression object, and feed X and y in as parameters\n",
    "lr.fit(X,y)\n",
    "\n",
    "# lr has a method called score.  Again, feed in X and y, and read the output. Save it in the variable score.  What is that number?  Compare it to statsmodels. \n",
    "score = lr.score(X,y)\n",
    "# that is the r_2.  It is the same as the Statsmodels R_2\n",
    "\n",
    "# lr also has attributes coef_ and intercept_. Save and compare to statsmodels\n",
    "beta = lr.coef_\n",
    "intercept = lr.intercept_\n",
    "#sklearn calculates the same coeficients and intercepts as statmodels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iterate: Build a better model - Add another numerical feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Features to the Model\n",
    "\n",
    "So far, all we have is a simple linear regression.  Let's start adding features to make it a multiple regression.\n",
    "\n",
    "Let's repeat the process of the highly positively correlated variables, but this time with the highly negatively correlated variables (based on looking at the correlation matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatively_correlated_cols = [\n",
    "    'life_expectancy',\n",
    "    'adult_mortality',\n",
    "    'hiv/aids',\n",
    "    'thinness__1-19_years',\n",
    "    'thinness_5-9_years'\n",
    "]\n",
    "negatively_correlated_df = df[negatively_correlated_cols]\n",
    "sns.pairplot(negatively_correlated_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`adult_mortality` seems most like a linear relationship.  Also, the two thinness metrics seem to be providing very similar information, so we almost certainly should not include both\n",
    "\n",
    "Let's proceed with adult mortality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataframe containing our three features of interest\n",
    "model_2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na's across all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the R-like formula into the variable\n",
    "formula = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model like we did above\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the summary table\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "# Create another dataframe containing our three features of interest\n",
    "model_2_df = df[['life_expectancy', 'schooling', 'adult_mortality']].copy()\n",
    "\n",
    "# Drop na's across all columns\n",
    "model_2_df.dropna(inplace=True)\n",
    "\n",
    "# save the R-like formula into the variable\n",
    "formula = 'life_expectancy~schooling+adult_mortality'\n",
    "\n",
    "# train the model like we did above\n",
    "model_2 = ols(formula=formula, data=model_2_df).fit()\n",
    "\n",
    "# print out the summary\n",
    "model_2.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did the r_2 improve? \n",
    "Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "'Adding another feature improved the r-squared from 0.565 to 0.714'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check the assumptions like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linearity\n",
    "\n",
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "rainbow_statistic, rainbow_p_value = linear_rainbow(model_2)\n",
    "print(\"Rainbow statistic:\", rainbow_statistic)\n",
    "print(\"Rainbow p-value:\", rainbow_p_value)\n",
    "\n",
    "'Assuming an alpha of 0.05, we are no longer violating the linearity assumption (just barely)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normality\n",
    "\n",
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "'''The Jarque-Bera (JB) output has gotten worse. We are still violating the normality assumption.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Homoscadasticity\n",
    "\n",
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "y_hat = model_2.predict()\n",
    "model_2_resids = model_2.resid\n",
    "\n",
    "fig4, ax4 = plt.subplots()\n",
    "ax4.set(xlabel=\"Predicted Life Expectancy\",\n",
    "        ylabel=\"Residuals (Actual - Predicted Life Expectancy)\")\n",
    "ax4.scatter(x=y_hat, y=model_2_resids, color=\"blue\", alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y-y_hat, model_2_df[[\"schooling\", \"adult_mortality\"]])\n",
    "print(\"Lagrange Multiplier p-value:\", lm_p_value)\n",
    "print(\"F-statistic p-value:\", f_p_value)\n",
    "\n",
    "'''Both visually and numerically, we can see some improvement. \n",
    "But we are still violating this assumption to a statistically significant degree.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence\n",
    "\n",
    "You might have noticed in the regression output that there was a warning about the condition number being high. The condition number is a measure of stability of the matrix used for computing the regression (we'll discuss this more in the next module), and a number above 30 can indicate strong multicollinearity. Our output is way higher than that.\n",
    "\n",
    "A different (more generous) measure of multicollinearity is the variance inflation factor. It is available from the outlier influence submodule of StatsModels.\n",
    "\n",
    "Run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = model_2_df[[\"schooling\", \"adult_mortality\"]].values\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"VIF\"] = [variance_inflation_factor(rows, i) for i in range(2)]\n",
    "vif_df[\"feature\"] = [\"schooling\", \"adult_mortality\"]\n",
    "\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"rule of thumb\" for VIF is that 5 is too high.  Given the output above, it's reasonable to say that we are not violating the independence assumption, despite the high condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Iterate: Build a better model - Add a categorical feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is less realistic than the previous steps, but is good for demonstartion purposes.\n",
    "\n",
    "In this dataset, we have a lot of numeric values (everything in that correlation matrix), but there are a few that aren't.  One example is `Status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have created a dataframe with the \"life_expectancy\", \"schooling\", \"adult_mortality\", \"status\"] columns\n",
    "model_3_df = df[[\"life_expectancy\", \"schooling\", \"adult_mortality\", \"status\"]].copy()\n",
    "\n",
    "# Drop NA's\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect value counts  of the status column\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "# Create a dataframe with the \"life_expectancy\", \"schooling\", \"adult_mortality\", \"status\"] columns\n",
    "model_3_df = df[[\"life_expectancy\", \"schooling\", \"adult_mortality\", \"status\"]].copy()\n",
    "\n",
    "# Drop NA's\n",
    "model_3_df.dropna(inplace=True)\n",
    "\n",
    "# Inspect value counts  of the status column\n",
    "model_3_df[\"status\"].value_counts()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out what Seaborn's catplot does\n",
    "# https://seaborn.pydata.org/generated/seaborn.catplot.html\n",
    "\n",
    "# Plot status vs life expectancy.  Choose a kind of plot to pass into the kind parameter\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot status vs life expectancy.  Choose a kind of plot to pass into the kind parameter\n",
    "#__SOLUTION__\n",
    "sns.catplot(x=\"status\", y=\"life_expectancy\", data=model_3_df, kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is a difference between the two groups that might be useful to include\n",
    "\n",
    "There are only two categories, so we only need a `LabelEncoder` that will convert the labels into 1s and 0s.  If there were more than two categories, we would use a `OneHotEncoder`, which would create multiple columns out of a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# instantiate and instance of LabelEncoder\n",
    "label_encoder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the \"status\" column of the model_3_df to the fit_transform() method of the Label Encoder\n",
    "status_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# instantiate and instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Pass the \"status\" column of the model_3_df to the fit_transform() method of the Label Encoder\n",
    "status_labels = label_encoder.fit_transform(model_3_df[\"status\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below.  The category Developing/Developed has been transformed to a binary\n",
    "np.unique(status_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below to see the classes associated with 1 and 0\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is telling us that \"Developed\" is encoded as 0 and \"Developing\" is encoded as 1.  This means that \"Developed\" is assumed at the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the status labels array to the model_df as a column \n",
    "model_3_df[\"status_encoded\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the status column\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Add the status labels array to the model_df as a column \n",
    "model_3_df[\"status_encoded\"] = status_labels\n",
    "model_3_df.drop(\"status\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the 3rd model\n",
    "\n",
    "# assign the new formula\n",
    "\n",
    "formula=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the new model\n",
    "model_3 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Fit the 3rd model\n",
    "\n",
    "# assign the new formula\n",
    "\n",
    "formula=\"life_expectancy~\" + \"+\".join(model_3_df.iloc[:,1:].columns)\n",
    "\n",
    "# fit the new model\n",
    "model_3 = ols(formula=formula, data=model_3_df).fit()\n",
    "\n",
    "# print the summary\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Model Evaluation\n",
    "\n",
    "Did the R_squared improve?\n",
    "\n",
    "Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Did the R_squared improve\n",
    "\"Adding another feature improved the r-squared a tiny bit from 0.714 to 0.718\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at the model assumptions again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainbow_statistic, rainbow_p_value = linear_rainbow(model_3)\n",
    "print(\"Rainbow statistic:\", rainbow_statistic)\n",
    "print(\"Rainbow p-value:\", rainbow_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did linearity improve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_SOLUTION__\n",
    "'Another small improvement'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normality\n",
    "Did our errors become more normally distributed?\n",
    "\n",
    "Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "#### Normality\n",
    "\n",
    "'''\n",
    "The **Jarque-Bera (JB)** output has gotten slightly better.  \n",
    "But we are still violating the normality assumption.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homoscadasticity\n",
    "\n",
    "Did our homoscadasticity improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_resids = None\n",
    "y_hat = None\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(y_hat, model_3_resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y-y_hat, model_3_df[[\"schooling\", \"adult_mortality\", \"status_encoded\"]])\n",
    "print(\"Lagrange Multiplier p-value:\", lm_p_value)\n",
    "print(\"F-statistic p-value:\", f_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#__SOLUTION__\n",
    "\n",
    "model_3_resids = model_3.resid\n",
    "y_hat = model_3.predict()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(y_hat, model_3_resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y-y_hat, model_3_df[[\"schooling\", \"adult_mortality\", \"status_encoded\"]])\n",
    "print(\"Lagrange Multiplier p-value:\", lm_p_value)\n",
    "print(\"F-statistic p-value:\", f_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "'''This metric got worse, although the plot looks fairly similar'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = model_3_df[[\"schooling\", \"adult_mortality\", \"status_encoded\"]].values\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"VIF\"] = [variance_inflation_factor(rows, i) for i in range(3)]\n",
    "vif_df[\"feature\"] = [\"schooling\", \"adult_mortality\", \"status_encoded\"]\n",
    "\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the above output tell you?\n",
    "\n",
    "Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\"\"\"The VIF metrics are getting higher, which means that there is stronger multicollinearity.  \n",
    "But we have still not exceeded the threshold of 5.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you will find an example summary of how one might use the linear regression models shown above to address the questions posed at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We started with a baseline model where the only input feature was Schooling. Our baseline model had an r-squared of 0.565. This model violated the linearity (p < 0.001), normality (p < 0.001), and homoscadasticity (p < 0.001) assumptions of linear regression. The independence assumption was met by default because there was only one input feature.\n",
    "\n",
    "The final model for this lesson had three input features: Schooling, Adult_Mortality, and Status_Encoded. It had an r-squared of 0.718. This model did not violate the linearity assumption (p = 0.084), but it did violate the normality (p < 0.001) and homoscedasticity (p < 0.001) assumptions. Based on the variance inflaction factor metric, it did not violate the independence assumption.\n",
    "\n",
    "We are able to address the following questions from above:\n",
    "\n",
    "1. Do various predicting factors which have been chosen initially really affect the Life expectancy? What are the predicting variables actually affecting the life expectancy?\n",
    "\n",
    "With only 3 features we are able to explain about 71% of the variance in life expectancy. This indicates that these factors truly are explanatory. More analysis is required to understand how much additional explanatory power would be provided by incorporating additional features into the model.\n",
    "\n",
    "3. How do Infant and Adult mortality rates affect life expectancy?\n",
    "\n",
    "So far we have only investigated adult mortality. The adult mortality rate (\"probability of dying between 15 and 60 years per 1000 population\") has a negative correlation with life expectancy. For each increase of 1 in the adult mortality rate, life expectancy decreases by about .03 years.\n",
    "\n",
    "5. What is the impact of schooling on the lifespan of humans?\n",
    "\n",
    "In our latest model, we find that each additional year of average schooling is associated with 1.4 years of added life expectancy. However it is challenging to interpret whether it is schooling that is actually having the impact. Schooling is highly correlated with Income_Composition_of_Resources (\"Human Development Index in terms of income composition of resources\") so it is very possible that schooling is the result of some underlying factor that also impacts life expectancy, rather than schooling impacting life expectancy directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Things we have not done in this lesson, but that you should consider in your project:  \n",
    "\n",
    "- More robust cleaning (possible imputation of missing values, principled exclusion of some data)  \n",
    "- Feature scaling  \n",
    "- Nearest-neighbors approach (requires more complex feature engineering)  \n",
    "- Pulling information from external resources  \n",
    "- Removing independent variables if you determine that they are causing too high of multicollinearity  \n",
    "- Setting up functions so the code is not so repetitive  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
